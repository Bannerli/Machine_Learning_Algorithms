{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autograd functionally\n",
    "import autograd.numpy as np\n",
    "from autograd.misc.flatten import flatten_func\n",
    "from autograd import grad as compute_grad\n",
    "\n",
    "# import various other libraries\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this is needed to compensate for %matplotl+ib notebook's tendancy lotted inline\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.autolayout\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent function\n",
    "def gradient_descent(g, w ,alpha, max_its, beta):\n",
    "    # flatten the input function, create gradient based on flat function\n",
    "    g_flat, unflatten, w = flatten_func(g, w)\n",
    "    grad = compute_grad(g_flat)\n",
    "\n",
    "    # record history\n",
    "    w_hist = []\n",
    "    # push the first w\n",
    "    w_hist.append(unflatten(w))\n",
    "\n",
    "    # start gradient descent loop\n",
    "    z = np.zeros(np.shape(w))  # momentum term\n",
    "\n",
    "    # over the line\n",
    "    for k in range(max_its):\n",
    "        # plug in value into func and derivative\n",
    "        grad_eval = grad(w)\n",
    "        grad_eval.shape = np.shape(w)\n",
    "\n",
    "        # take descent step with momentum\n",
    "        z = beta * z + grad_eval\n",
    "        w = w -alpha * z\n",
    "\n",
    "        # record weight update\n",
    "        w_hist.append(unflatten(w))\n",
    "\n",
    "    return w_hist\n",
    "\n",
    "def normalize(data, data_mean, data_std):\n",
    "    normalized = (data - data_mean)/data_std\n",
    "    return normalized\n",
    "\n",
    "def model(x ,w):\n",
    "    # feature transformations\n",
    "    f = w[0] + np.dot(x, w[1:])\n",
    "    return f\n",
    "\n",
    "def plotting(cost_hist, ylabelName, label):\n",
    "    figure, axes = plt.subplots(1,1, figsize = (6,5))\n",
    "    axes.plot(range(len(cost_hist)),cost_hist, label = label, linestyle = \"solid\")\n",
    "    axes.set_xlabel(\"iterations\")\n",
    "    axes.set_ylabel(ylabelName)\n",
    "    axes.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvname = \"credit_dataset.csv\"\n",
    "data = np.loadtxt(csvname, delimiter = ',')\n",
    "data = data.T\n",
    "x = data[:,:-1]\n",
    "y = data[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do the normalization of these data matrix\n",
    "x_means = np.mean(x, axis = 0)\n",
    "x_stds = np.std(x, axis = 0)\n",
    "# normalize the input data\n",
    "x_normed = normalize(x, x_means, x_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    a_1 = np.array(x>=0, dtype = np.int)\n",
    "    a_2 = np.array(x<0, dtype = np.int)\n",
    "    return a_1*1 + a_2*(-1)\n",
    "\n",
    "def softmax_cost(w):\n",
    "    cost = np.sum(np.log(1+np.exp(-y*model(x_normed,w))))\n",
    "    return cost/float(np.size(y))\n",
    "\n",
    "def perceptron_cost(w):\n",
    "    cost = np.sum(np.maximum(0,-y*model(x_normed,w)))\n",
    "    return cost/float(np.size(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "alpha = 0.05\n",
    "max_its = 1000\n",
    "beta = 0\n",
    "w_init = np.random.randn(x.shape[1]+1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent, create cost function history\n",
    "weight_history = gradient_descent(softmax_cost, w_init, alpha, max_its,beta)\n",
    "# use MSE to validate the regression quality\n",
    "cost_history = [softmax_cost(v) for v in weight_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_w = weight_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x,y):\n",
    "    result = np.array(x == y, dtype = int)\n",
    "    number = np.sum(result)\n",
    "    return number/np.size(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.779"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(sign(model(x_normed,best_w)),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = sign(model(x_normed,best_w))\n",
    "y_true = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [y_pred[i][0] for i in range(np.size(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we show the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148, 152],\n",
       "       [ 69, 631]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix = confusion_matrix(y_true, y_pred)\n",
    "confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (confusionMatrix[0][0]+confusionMatrix[1][1])/(np.sum(confusionMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.779"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we use the softmax cost as the optimizer. \n",
    "We also use two ways to get the accuracy.\n",
    "Now we validate the accuracy, is 0.779, which is around 0.75 but better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
